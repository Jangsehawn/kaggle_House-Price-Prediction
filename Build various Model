{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Build various Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "import time\n",
    "import gc\n",
    "import random as rn\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "from datetime import date, datetime, timedelta\n",
    "from functools import wraps\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, Imputer, StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor, ExtraTreesRegressor\n",
    "from sklearn.linear_model import ElasticNet, Lasso, Ridge\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cat\n",
    "\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Embedding, Reshape, Concatenate, Input, Flatten\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf\n",
    "# from tensorflow import set_random_seed\n",
    "\n",
    "import scipy\n",
    "from scipy import stats\n",
    "from scipy.stats import norm, skew #for some statistics\n",
    "from scipy.cluster import hierarchy as hc\n",
    "from scipy.special import boxcox1p\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0) dataset 준비\n",
    "\n",
    "### 0> 기타 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RANDOM_SEED = 42\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "RANDOM_SEED = 0\n",
    "rn.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "\n",
    "#session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "#sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "#K.set_session(sess)\n",
    "\n",
    "def plot_numeric_for_regression(df, field, target_field='price'):\n",
    "    df = df[df[field].notnull()]\n",
    "\n",
    "    fig = plt.figure(figsize = (16, 7))\n",
    "    ax1 = plt.subplot(121)\n",
    "    \n",
    "    sns.distplot(df[df['data'] == 'train'][field], label='Train', hist_kws={'alpha': 0.5}, ax=ax1)\n",
    "    sns.distplot(df[df['data'] == 'test'][field], label='Test', hist_kws={'alpha': 0.5}, ax=ax1)\n",
    "\n",
    "    plt.xlabel(field)\n",
    "    plt.ylabel('Density')\n",
    "    plt.legend()\n",
    "    \n",
    "    ax2 = plt.subplot(122)\n",
    "    \n",
    "    df_copy = df[df['data'] == 'train'].copy()\n",
    "\n",
    "    sns.scatterplot(x=field, y=target_field, data=df_copy, ax=ax2)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "def plot_categorical_for_regression(df, field, target_field='price', show_missing=True, missing_value='NA'):\n",
    "    df_copy = df.copy()\n",
    "    if show_missing: df_copy[field] = df_copy[field].fillna(missing_value)\n",
    "    df_copy = df_copy[df_copy[field].notnull()]\n",
    "\n",
    "    ax1_param = 121\n",
    "    ax2_param = 122\n",
    "    fig_size = (16, 7)\n",
    "    if df_copy[field].nunique() > 30:\n",
    "        ax1_param = 211\n",
    "        ax2_param = 212\n",
    "        fig_size = (16, 10)\n",
    "    \n",
    "    fig = plt.figure(figsize = fig_size)\n",
    "    ax1 = plt.subplot(ax1_param)\n",
    "    \n",
    "    sns.countplot(x=field, hue='data', order=np.sort(df_copy[field].unique()), data=df_copy)\n",
    "    plt.xticks(rotation=90, fontsize=11)\n",
    "    \n",
    "    ax2 = plt.subplot(ax2_param)\n",
    "    \n",
    "    df_copy = df_copy[df_copy['data'] == 'train']\n",
    "\n",
    "    sns.boxplot(x=field, y=target_field, data=df_copy, order=np.sort(df_copy[field].unique()), ax=ax2)\n",
    "    plt.xticks(rotation=90, fontsize=11)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def get_prefix(group_col, target_col, prefix=None):\n",
    "    if isinstance(group_col, list) is True:\n",
    "        g = '_'.join(group_col)\n",
    "    else:\n",
    "        g = group_col\n",
    "    if isinstance(target_col, list) is True:\n",
    "        t = '_'.join(target_col)\n",
    "    else:\n",
    "        t = target_col\n",
    "    if prefix is not None:\n",
    "        return prefix + '_' + g + '_' + t\n",
    "    return g + '_' + t\n",
    "    \n",
    "def groupby_helper(df, group_col, target_col, agg_method, prefix_param=None):\n",
    "    try:\n",
    "        prefix = get_prefix(group_col, target_col, prefix_param)\n",
    "        #print(group_col, target_col, agg_method)\n",
    "        group_df = df.groupby(group_col)[target_col].agg(agg_method)\n",
    "        group_df.columns = ['{}_{}'.format(prefix, m) for m in agg_method]\n",
    "    except BaseException as e:\n",
    "        print(e)\n",
    "    return group_df.reset_index()\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "def rmse_exp(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(np.expm1(y_true), np.expm1(y_pred)))\n",
    "\n",
    "def time_decorator(func): \n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        print(\"\\nStartTime: \", datetime.now() + timedelta(hours=9))\n",
    "        start_time = time.time()\n",
    "        \n",
    "        df = func(*args, **kwargs)\n",
    "        \n",
    "        print(\"EndTime: \", datetime.now() + timedelta(hours=9))  \n",
    "        print(\"TotalTime: \", time.time() - start_time)\n",
    "        return df\n",
    "        \n",
    "    return wrapper\n",
    "\n",
    "class SklearnWrapper(object):\n",
    "    def __init__(self, clf, params=None, **kwargs):\n",
    "        #if isinstance(SVR) is False:\n",
    "        #    params['random_state'] = kwargs.get('seed', 0)\n",
    "        self.clf = clf(**params)\n",
    "        self.is_classification_problem = True\n",
    "        self.use_avg_oof = kwargs.get('use_avg_oof', False)\n",
    "    @time_decorator\n",
    "    def train(self, x_train, y_train, x_cross=None, y_cross=None):\n",
    "        if len(np.unique(y_train)) > 30:\n",
    "            self.is_classification_problem = False\n",
    "            \n",
    "        self.clf.fit(x_train, y_train)\n",
    "\n",
    "    def predict(self, x):\n",
    "        if self.is_classification_problem is True:\n",
    "            return self.clf.predict_proba(x)[:,1]\n",
    "        else:\n",
    "            return self.clf.predict(x)\n",
    "    \n",
    "class XgbWrapper(object):\n",
    "    def __init__(self, params=None, **kwargs):\n",
    "        self.param = params\n",
    "        self.use_avg_oof = kwargs.get('use_avg_oof', False)\n",
    "        self.num_rounds = kwargs.get('num_rounds', 1000)\n",
    "        self.early_stopping = kwargs.get('ealry_stopping', 100)\n",
    "\n",
    "        self.eval_function = kwargs.get('eval_function', None)\n",
    "        self.verbose_eval = kwargs.get('verbose_eval', 100)\n",
    "        self.best_round = 0\n",
    "    \n",
    "    @time_decorator\n",
    "    def train(self, x_train, y_train, x_cross=None, y_cross=None):\n",
    "        need_cross_validation = True\n",
    "       \n",
    "        if isinstance(y_train, pd.DataFrame) is True:\n",
    "            y_train = y_train[y_train.columns[0]]\n",
    "            if y_cross is not None:\n",
    "                y_cross = y_cross[y_cross.columns[0]]\n",
    "\n",
    "        if x_cross is None:\n",
    "            dtrain = xgb.DMatrix(x_train, label=y_train, silent= True)\n",
    "            train_round = self.best_round\n",
    "            if self.best_round == 0:\n",
    "                train_round = self.num_rounds\n",
    "            \n",
    "            self.clf = xgb.train(self.param, dtrain, train_round)\n",
    "            del dtrain\n",
    "        else:\n",
    "            dtrain = xgb.DMatrix(x_train, label=y_train, silent=True)\n",
    "            dvalid = xgb.DMatrix(x_cross, label=y_cross, silent=True)\n",
    "            watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n",
    "\n",
    "            self.clf = xgb.train(self.param, dtrain, self.num_rounds, watchlist, feval=self.eval_function,\n",
    "                                 early_stopping_rounds=self.early_stopping,\n",
    "                                 verbose_eval=self.verbose_eval)\n",
    "            self.best_round = max(self.best_round, self.clf.best_iteration)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.clf.predict(xgb.DMatrix(x), ntree_limit=self.best_round)\n",
    "\n",
    "    def get_params(self):\n",
    "        return self.param    \n",
    "    \n",
    "class LgbmWrapper(object):\n",
    "    def __init__(self, params=None, **kwargs):\n",
    "        self.param = params\n",
    "        self.use_avg_oof = kwargs.get('use_avg_oof', False)\n",
    "        self.num_rounds = kwargs.get('num_rounds', 1000)\n",
    "        self.early_stopping = kwargs.get('ealry_stopping', 100)\n",
    "\n",
    "        self.eval_function = kwargs.get('eval_function', None)\n",
    "        self.verbose_eval = kwargs.get('verbose_eval', 100)\n",
    "        self.best_round = 0\n",
    "        \n",
    "    @time_decorator\n",
    "    def train(self, x_train, y_train, x_cross=None, y_cross=None):\n",
    "        \"\"\"\n",
    "        x_cross or y_cross is None\n",
    "        -> model train limted num_rounds\n",
    "        \n",
    "        x_cross and y_cross is Not None\n",
    "        -> model train using validation set\n",
    "        \"\"\"\n",
    "        if isinstance(y_train, pd.DataFrame) is True:\n",
    "            y_train = y_train[y_train.columns[0]]\n",
    "            if y_cross is not None:\n",
    "                y_cross = y_cross[y_cross.columns[0]]\n",
    "\n",
    "        if x_cross is None:\n",
    "            dtrain = lgb.Dataset(x_train, label=y_train, silent= True)\n",
    "            train_round = self.best_round\n",
    "            if self.best_round == 0:\n",
    "                train_round = self.num_rounds\n",
    "                \n",
    "            self.clf = lgb.train(self.param, train_set=dtrain, num_boost_round=train_round)\n",
    "            del dtrain   \n",
    "        else:\n",
    "            dtrain = lgb.Dataset(x_train, label=y_train, silent=True)\n",
    "            dvalid = lgb.Dataset(x_cross, label=y_cross, silent=True)\n",
    "            self.clf = lgb.train(self.param, train_set=dtrain, num_boost_round=self.num_rounds, valid_sets=[dtrain, dvalid],\n",
    "                                  feval=self.eval_function, early_stopping_rounds=self.early_stopping,\n",
    "                                  verbose_eval=self.verbose_eval)\n",
    "            self.best_round = max(self.best_round, self.clf.best_iteration)\n",
    "            del dtrain, dvalid\n",
    "            \n",
    "        gc.collect()\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return self.clf.predict(x, num_iteration=self.clf.best_iteration)\n",
    "    \n",
    "    def plot_importance(self, importance_type='gain', max_num_features=20):\n",
    "        lgb.plot_importance(self.clf, importance_type=importance_type, max_num_features=max_num_features, height=0.7, figsize=(10,30))\n",
    "        plt.show()\n",
    "        \n",
    "    def get_params(self):\n",
    "        return self.param\n",
    "\n",
    "class CatWrapper(object):\n",
    "    def __init__(self, params=None, **kwargs):\n",
    "        self.param = params\n",
    "        self.use_avg_oof = kwargs.get('use_avg_oof', False)\n",
    "        self.num_rounds = kwargs.get('num_rounds', 1000)\n",
    "        self.param['iterations'] = kwargs.get('num_rounds', 1000)\n",
    "        self.early_stopping = kwargs.get('ealry_stopping', 100)\n",
    "\n",
    "        self.eval_function = kwargs.get('eval_function', None)\n",
    "        self.verbose_eval = kwargs.get('verbose_eval', 100)\n",
    "        self.best_round = 0\n",
    "        \n",
    "    @time_decorator\n",
    "    def train(self, x_train, y_train, x_cross=None, y_cross=None, cat_features=None):\n",
    "        \"\"\"\n",
    "        x_cross or y_cross is None\n",
    "        -> model train limted num_rounds\n",
    "        \n",
    "        x_cross and y_cross is Not None\n",
    "        -> model train using validation set\n",
    "        \"\"\"\n",
    "        if isinstance(y_train, pd.DataFrame) is True:\n",
    "            y_train = y_train[y_train.columns[0]]\n",
    "            if y_cross is not None:\n",
    "                y_cross = y_cross[y_cross.columns[0]]\n",
    "\n",
    "        if x_cross is None:\n",
    "            dtrain = cat.Pool(x_train, y_train, cat_features=cat_features)\n",
    "            train_round = self.best_round\n",
    "            if self.best_round == 0:\n",
    "                train_round = self.num_rounds\n",
    "                \n",
    "            self.clf = cat.CatBoost(params=self.param)\n",
    "            self.clf.fit(dtrain, verbose_eval=self.verbose_eval)\n",
    "            del dtrain   \n",
    "        else:\n",
    "            dtrain = cat.Pool(x_train, y_train, cat_features=cat_features)\n",
    "            dvalid = cat.Pool(x_cross, y_cross, cat_features=cat_features)\n",
    "            \n",
    "            self.clf = cat.CatBoost(params=self.param)\n",
    "            self.clf.fit(dtrain, eval_set=[dvalid], early_stopping_rounds=self.early_stopping, verbose_eval=self.verbose_eval)\n",
    "            self.best_round = max(self.best_round, self.clf.best_iteration_)\n",
    "            del dtrain, dvalid\n",
    "            \n",
    "        gc.collect()\n",
    "    \n",
    "    def predict(self, x):\n",
    "        if self.clf.best_iteration_ is None: return self.clf.predict(x, ntree_end=self.best_round)\n",
    "        else: return self.clf.predict(x, ntree_end=self.clf.best_iteration_)\n",
    "        \n",
    "    def get_params(self):\n",
    "        return self.param\n",
    "    \n",
    "class KerasWrapper(object):\n",
    "    def __init__(self, model_func, params=None, **kwargs):\n",
    "        self.model_func = model_func\n",
    "        self.param = params\n",
    "        self.use_avg_oof = kwargs.get('use_avg_oof', False)\n",
    "        self.epochs = kwargs.get('epochs', 20)\n",
    "        self.batch_size = kwargs.get('batch_size', 16)\n",
    "        self.callbacks = kwargs.get('callbacks', None)\n",
    "        self.shuffle = kwargs.get('shuffle', True)\n",
    "        self.best_epochs = 0\n",
    "    @time_decorator\n",
    "    def train(self, x_train, y_train, x_cross=None, y_cross=None):\n",
    "        self.model = self.model_func(x_train.shape[1])\n",
    "        if x_cross is None:\n",
    "            train_epochs = self.best_epochs\n",
    "            if self.best_epochs == 0:\n",
    "                train_epochs = self.epochs\n",
    "                \n",
    "            self.model.fit(x_train, y_train, epochs=train_epochs, batch_size=self.batch_size,\n",
    "                           shuffle=self.shuffle, callbacks=self.callbacks, verbose=0)\n",
    "        else:\n",
    "            hist = self.model.fit(x_train, y_train, epochs=self.epochs, batch_size=self.batch_size,\n",
    "                                  shuffle=self.shuffle, validation_data=(x_cross, y_cross),\n",
    "                                  callbacks=self.callbacks, verbose=0)\n",
    "            self.best_epochs = max(self.best_epochs, len(hist.history['val_loss']))\n",
    "\n",
    "    def predict(self, x):\n",
    "        if isinstance(x, pd.DataFrame):\n",
    "            return self.model.predict(x.values).ravel()\n",
    "        else:\n",
    "            return self.model.predict(x).ravel()\n",
    "    \n",
    "    def get_params(self):\n",
    "        return self.param\n",
    "\n",
    "class KerasEmbeddingWrapper(object):\n",
    "    def __init__(self, model_func, params=None, **kwargs):\n",
    "        self.model_func = model_func\n",
    "        self.param = params\n",
    "        self.use_avg_oof = kwargs.get('use_avg_oof', False)\n",
    "        self.epochs = kwargs.get('epochs', 20)\n",
    "        self.batch_size = kwargs.get('batch_size', 16)\n",
    "        self.callbacks = kwargs.get('callbacks', None)\n",
    "        self.embedding_cols = kwargs.get('embedding_cols', None)\n",
    "        self.shuffle = kwargs.get('shuffle', True)\n",
    "        self.best_epochs = 0\n",
    "    @time_decorator\n",
    "    def train(self, x_train, y_train, x_cross=None, y_cross=None):\n",
    "        non_embedding_cols = [col for col in x_train.columns if col not in self.embedding_cols]\n",
    "        self.model = self.model_func(x_train, self.embedding_cols)\n",
    "        if x_cross is None:\n",
    "            train_epochs = self.best_epochs\n",
    "            if self.best_epochs == 0:\n",
    "                train_epochs = self.epochs\n",
    "                \n",
    "            x_tr_list = []\n",
    "            x_tr_list.append(x_train[non_embedding_cols])\n",
    "            for col in self.embedding_cols:\n",
    "                x_tr_list.append(x_train[col])\n",
    "            self.model.fit(x_tr_list, y_train, epochs=train_epochs, batch_size=self.batch_size,\n",
    "                           shuffle=self.shuffle, callbacks=self.callbacks, verbose=0)\n",
    "        else:\n",
    "            x_tr_list = []\n",
    "            x_tr_list.append(x_train[non_embedding_cols])\n",
    "            for col in self.embedding_cols:\n",
    "                x_tr_list.append(x_train[col])\n",
    "\n",
    "            x_cr_list = []\n",
    "            x_cr_list.append(x_cross[non_embedding_cols])\n",
    "            for col in self.embedding_cols:\n",
    "                x_cr_list.append(x_cross[col])\n",
    "            hist = self.model.fit(x_tr_list, y_train, epochs=self.epochs, batch_size=self.batch_size, shuffle=self.shuffle,\n",
    "                                  validation_data=(x_cr_list, y_cross), callbacks=self.callbacks, verbose=0)\n",
    "            self.best_epochs = max(self.best_epochs, len(hist.history['val_loss']))\n",
    "\n",
    "    def predict(self, x):\n",
    "        non_embedding_cols = [col for col in x.columns if col not in self.embedding_cols]\n",
    "        x_list = []\n",
    "        x_list.append(x[non_embedding_cols])\n",
    "        for col in self.embedding_cols:\n",
    "            x_list.append(x[col])\n",
    "        return self.model.predict(x_list).ravel()\n",
    "    \n",
    "    def get_params(self):\n",
    "        return self.param\n",
    "\n",
    "\n",
    "@time_decorator\n",
    "def get_oof(clf, x_train, y_train, x_test, eval_func, **kwargs):\n",
    "    nfolds = kwargs.get('NFOLDS', 5)\n",
    "    kfold_shuffle = kwargs.get('kfold_shuffle', True)\n",
    "    kfold_random_state = kwargs.get('kfold_random_state', 0)\n",
    "    stratified_kfold_ytrain = kwargs.get('stratifed_kfold_y_value', None)\n",
    "    ntrain = x_train.shape[0]\n",
    "    ntest = x_test.shape[0]\n",
    "    \n",
    "    kf_split = None\n",
    "    if stratified_kfold_ytrain is None:\n",
    "        kf = KFold(n_splits=nfolds, shuffle=kfold_shuffle, random_state=kfold_random_state)\n",
    "        kf_split = kf.split(x_train)\n",
    "    else:\n",
    "        kf = StratifiedKFold(n_splits=nfolds, shuffle=kfold_shuffle, random_state=kfold_random_state)\n",
    "        kf_split = kf.split(x_train, stratified_kfold_ytrain)\n",
    "        \n",
    "    oof_train = np.zeros((ntrain,))\n",
    "    oof_test = np.zeros((ntest,))\n",
    "\n",
    "    cv_sum = 0\n",
    "    \n",
    "    # before running model, print model param\n",
    "    # lightgbm model and xgboost model use get_params()\n",
    "    try:\n",
    "        if clf.clf is not None:\n",
    "            print(clf.clf)\n",
    "    except:\n",
    "        print(clf)\n",
    "        print(clf.get_params())\n",
    "\n",
    "    for i, (train_index, cross_index) in enumerate(kf_split):\n",
    "        x_tr, x_cr = None, None\n",
    "        y_tr, y_cr = None, None\n",
    "        if isinstance(x_train, pd.DataFrame):\n",
    "            x_tr, x_cr = x_train.iloc[train_index], x_train.iloc[cross_index]\n",
    "            y_tr, y_cr = y_train.iloc[train_index], y_train.iloc[cross_index]\n",
    "        else:\n",
    "            x_tr, x_cr = x_train[train_index], x_train[cross_index]\n",
    "            y_tr, y_cr = y_train[train_index], y_train[cross_index]\n",
    "\n",
    "        clf.train(x_tr, y_tr, x_cr, y_cr)\n",
    "        \n",
    "        oof_train[cross_index] = clf.predict(x_cr)\n",
    "        if hasattr(clf, 'use_avg_oof') and clf.use_avg_oof:\n",
    "            oof_test += clf.predict(x_test)/nfolds\n",
    "\n",
    "        cv_score = eval_func(y_cr, oof_train[cross_index])\n",
    "        \n",
    "        print('Fold %d / ' % (i+1), 'CV-Score: %.6f' % cv_score)\n",
    "        cv_sum = cv_sum + cv_score\n",
    "        \n",
    "        del x_tr, x_cr, y_tr, y_cr\n",
    "        \n",
    "    gc.collect()\n",
    "    \n",
    "    score = cv_sum / nfolds\n",
    "    print(\"Average CV-Score: \", score)\n",
    "\n",
    "    # Using All Dataset, retrain\n",
    "    if not hasattr(clf, 'use_avg_oof') or clf.use_avg_oof is False:\n",
    "        clf.train(x_train, y_train)\n",
    "        oof_test = clf.predict(x_test)\n",
    "\n",
    "    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1), score\n",
    "\n",
    "@time_decorator\n",
    "def stacking(data_list, y_train, model_list, eval_func=None, nfolds=5, kfold_random_state=RANDOM_SEED):\n",
    "    \n",
    "    oof_train_list = []\n",
    "    oof_test_list = []\n",
    "    oof_cv_score_list = []\n",
    "    \n",
    "    for X_train, X_test in data_list:\n",
    "        print(X_train.shape, X_test.shape, y_train.shape)\n",
    "        for model in model_list:\n",
    "            oof_train, oof_test, oof_cv_score = get_oof(model, X_train, y_train, X_test, eval_func,\n",
    "                                                        NFOLDS=nfolds, kfold_random_state=kfold_random_state)\n",
    "            oof_train_list.append(oof_train)\n",
    "            oof_test_list.append(oof_test)\n",
    "            oof_cv_score_list.append(oof_cv_score)\n",
    "        \n",
    "    X_train_next = pd.DataFrame(np.concatenate(oof_train_list, axis=1))\n",
    "    X_test_next = pd.DataFrame(np.concatenate(oof_test_list, axis=1))\n",
    "    \n",
    "    print(X_train_next.shape, X_test_next.shape)\n",
    "    \n",
    "    return X_train_next, X_test_next, oof_cv_score_list\n",
    "\n",
    "def haversine_array(lat1, lng1, lat2, lng2): \n",
    "    lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2)) \n",
    "    AVG_EARTH_RADIUS = 6371 # in km \n",
    "    lat = lat2 - lat1 \n",
    "    lng = lng2 - lng1 \n",
    "    d = np.sin(lat * 0.5) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(lng * 0.5) ** 2 \n",
    "    h = 2 * AVG_EARTH_RADIUS * np.arcsin(np.sqrt(d)) \n",
    "    return h\n",
    "\n",
    "def bearing_array(lat1, lng1, lat2, lng2): \n",
    "    AVG_EARTH_RADIUS = 6371 # in km \n",
    "    lng_delta_rad = np.radians(lng2 - lng1) \n",
    "    lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2)) \n",
    "    y = np.sin(lng_delta_rad) * np.cos(lat2) \n",
    "    x = np.cos(lat1) * np.sin(lat2) - np.sin(lat1) * np.cos(lat2) * np.cos(lng_delta_rad) \n",
    "    return np.degrees(np.arctan2(y, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1> define function loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "\n",
    "def load_data(nb_1km=True, nb_3km=True, nb_5km=True,\n",
    "              n_5_nb=True, n_10_nb=True, n_20_nb=True,\n",
    "              original=True, do_scale=False, fix_skew=False, do_ohe=True):\n",
    "    train = pd.read_csv('./train.csv')\n",
    "    test = pd.read_csv('./test.csv')\n",
    "\n",
    "    train_copy = train.copy()\n",
    "    train_copy['data'] = 'train'\n",
    "    test_copy = test.copy()\n",
    "    test_copy['data'] = 'test'\n",
    "    test_copy['price'] = np.nan\n",
    "    \n",
    "    # remove outlier\n",
    "    train_copy = train_copy[~((train_copy['sqft_living'] > 12000) & (train_copy['price'] < 3000000))].reset_index(drop=True)\n",
    "\n",
    "    # concat train, test data to preprocess\n",
    "    data = pd.concat([train_copy, test_copy]).reset_index(drop=True)\n",
    "    data = data[train_copy.columns]\n",
    "    \n",
    "    # fix skew feature\n",
    "    skew_columns = ['price']\n",
    "\n",
    "    for c in skew_columns:\n",
    "        data[c] = np.log1p(data[c])\n",
    "    \n",
    "    if original:\n",
    "        # feature engineering\n",
    "        data['date'] = pd.to_datetime(data['date'])\n",
    "        data['yr_mo_sold'] = data['date'].dt.strftime('%Y-%m')\n",
    "        data['yr_sold'] = data['date'].dt.year\n",
    "        data['qt_sold'] = data['date'].dt.quarter\n",
    "        data['week_sold'] = data['date'].dt.week\n",
    "        data['dow_sold'] = data['date'].dt.dayofweek\n",
    "        data['yr_sold - yr_built'] = data['yr_sold'] - data['yr_built']\n",
    "        data['yr_sold - yr_renovated'] = data['yr_sold'] - data['yr_renovated']\n",
    "        data['yr_renovated - yr_built'] = data['yr_renovated'] - data['yr_built']\n",
    "        \n",
    "        data['yr_sold'] = data['yr_sold'].astype(str)\n",
    "        data['qt_sold'] = data['qt_sold'].astype(str)\n",
    "        data['week_sold'] = data['week_sold'].astype(str)\n",
    "        data['dow_sold'] = data['dow_sold'].astype(str)\n",
    "        data.drop(['date'], axis=1, inplace=True)\n",
    "\n",
    "        data['bedrooms + bathrooms'] = data['bedrooms'] + data['bathrooms']\n",
    "        data['bathrooms / bedrooms'] = data['bathrooms'] / data['bedrooms']\n",
    "        data.loc[np.isinf(data['bathrooms / bedrooms']), 'bathrooms / bedrooms'] = 0\n",
    "        data['bathrooms / bedrooms'].fillna(0, inplace=True)\n",
    "        data['sqft_living / bedrooms'] = data['sqft_living'] / data['bedrooms']\n",
    "        data.loc[np.isinf(data['sqft_living / bedrooms']), 'sqft_living / bedrooms'] = 0\n",
    "        data['sqft_living / bathrooms'] = data['sqft_living'] / data['bathrooms']\n",
    "        data.loc[np.isinf(data['sqft_living / bathrooms']), 'sqft_living / bathrooms'] = 0\n",
    "        data['sqft_living / floors'] = data['sqft_living'] / data['floors']\n",
    "        data.loc[np.isinf(data['sqft_living / floors']), 'sqft_living / floors'] = 0\n",
    "        data['sqft_lot / sqft_living'] = data['sqft_lot'] / data['sqft_living']\n",
    "        data['sqft_basement / sqft_above'] = data['sqft_basement'] / data['sqft_above']\n",
    "        data['sqft_lot15 / sqft_living15'] = data['sqft_lot15'] / data['sqft_living15']\n",
    "        data['has_basement'] = data['sqft_basement'] > 0\n",
    "        data['is_renovated'] = data['yr_renovated'] > 0\n",
    "        data['sqft_living_changed'] = data['sqft_living'] != data['sqft_living15']\n",
    "        data['sqft_lot_changed'] = data['sqft_lot'] != data['sqft_lot15']\n",
    "        data['sqft_living * grade'] = data['sqft_living'] * data['grade']\n",
    "        data['overall'] = data['grade'] + data['view'] + data['condition'] + data['waterfront'] + data['has_basement'] + data['is_renovated']\n",
    "        data['sqft_living * overall'] = data['sqft_living'] * data['overall']\n",
    "\n",
    "        data['zipcode'] = data['zipcode'].astype(str)\n",
    "        data['zipcode-3'] = data['zipcode'].str[2:3]\n",
    "        data['zipcode-4'] = data['zipcode'].str[3:4]\n",
    "        data['zipcode-5'] = data['zipcode'].str[4:5]\n",
    "        data['zipcode-34'] = data['zipcode'].str[2:4]\n",
    "        data['zipcode-45'] = data['zipcode'].str[3:5]\n",
    "        data['zipcode-35'] = data['zipcode-3'] + data['zipcode-5']\n",
    "\n",
    "        # pca for lat, long\n",
    "        coord = data[['lat','long']]\n",
    "        pca = PCA(n_components=2)\n",
    "        pca.fit(coord)\n",
    "\n",
    "        coord_pca = pca.transform(coord)\n",
    "\n",
    "        data['coord_pca1'] = coord_pca[:, 0]\n",
    "        data['coord_pca2'] = coord_pca[:, 1]\n",
    "\n",
    "        # kmeans for lat, long\n",
    "        kmeans = KMeans(n_clusters=72, random_state=RANDOM_SEED).fit(coord)\n",
    "        coord_cluster = kmeans.predict(coord)\n",
    "        data['coord_cluster'] = coord_cluster\n",
    "        data['coord_cluster'] = data['coord_cluster'].map(lambda x: 'cluster_' + str(x).rjust(2, '0'))\n",
    "\n",
    "        lat_med = data['lat'].median()\n",
    "        long_med = data['long'].median()\n",
    "\n",
    "        lat2 = data['lat'].values\n",
    "        long2 = data['long'].values\n",
    "\n",
    "        bearing_arr = bearing_array(lat_med, long_med, lat2, long2)\n",
    "\n",
    "        data['bearing_from_center'] = bearing_arr\n",
    "\n",
    "        qcut_count = 10\n",
    "        data['qcut_bearing'] = pd.qcut(data['bearing_from_center'], qcut_count, labels=range(qcut_count))\n",
    "        data['qcut_bearing'] = data['qcut_bearing'].astype(str)\n",
    "\n",
    "        # calculate grouped price\n",
    "        group_cols = ['grade','bedrooms','bathrooms','view','condition','waterfront']\n",
    "\n",
    "        for col in group_cols:\n",
    "            group_df = groupby_helper(data[data['data'] == 'train'], col, 'price', ['mean']).fillna(0)\n",
    "            data = data.merge(group_df, on=col, how='left').fillna(0)\n",
    "            \n",
    "        cat_cols = [\n",
    "            'yr_mo_sold','yr_sold','qt_sold','week_sold','dow_sold','coord_cluster',\n",
    "            'zipcode','zipcode-3','zipcode-4','zipcode-5','zipcode-34','zipcode-45','zipcode-35',\n",
    "            'qcut_bearing'\n",
    "        ]\n",
    "            \n",
    "        if do_ohe:\n",
    "            for col in cat_cols:\n",
    "                ohe_df = pd.get_dummies(data[[col]], prefix='ohe_'+col)\n",
    "                data.drop(col, axis=1, inplace=True)\n",
    "                data = pd.concat([data, ohe_df], axis=1)\n",
    "        else:\n",
    "            for col in cat_cols:\n",
    "                le = LabelEncoder()\n",
    "                data[col] = le.fit_transform(data[col])\n",
    "    else:\n",
    "        data = data[['id','price','data']]\n",
    "        \n",
    "    if nb_1km:\n",
    "        neighbor_1km_stat = pd.read_csv('./neighbor_1km_stat.csv')\n",
    "        data = data.merge(neighbor_1km_stat, on='id', how='left').fillna(0)\n",
    "        \n",
    "        if original:\n",
    "            data['sqft_living - nb_1km_sqft_living_mean'] = data['sqft_living'] - data['nb_1km_sqft_living_mean']\n",
    "            data['sqft_lot - nb_1km_sqft_lot_mean'] = data['sqft_lot'] - data['nb_1km_sqft_lot_mean']\n",
    "            data['bedrooms - nb_1km_bedrooms_mean'] = data['bedrooms'] - data['nb_1km_bedrooms_mean']\n",
    "            data['bathrooms - nb_1km_bathrooms_mean'] = data['bathrooms'] - data['nb_1km_bathrooms_mean']\n",
    "            data['grade - nb_1km_grade_mean'] = data['grade'] - data['nb_1km_grade_mean']\n",
    "            data['view - nb_1km_view_mean'] = data['view'] - data['nb_1km_view_mean']\n",
    "            data['condition - nb_1km_condition_mean'] = data['condition'] - data['nb_1km_condition_mean']\n",
    "    if nb_3km:\n",
    "        neighbor_3km_stat = pd.read_csv('./neighbor_3km_stat.csv')\n",
    "        data = data.merge(neighbor_3km_stat, on='id', how='left').fillna(0)\n",
    "        \n",
    "        if original:\n",
    "            data['sqft_living - nb_3km_sqft_living_mean'] = data['sqft_living'] - data['nb_3km_sqft_living_mean']\n",
    "            data['sqft_lot - nb_3km_sqft_lot_mean'] = data['sqft_lot'] - data['nb_3km_sqft_lot_mean']\n",
    "            data['bedrooms - nb_3km_bedrooms_mean'] = data['bedrooms'] - data['nb_3km_bedrooms_mean']\n",
    "            data['bathrooms - nb_3km_bathrooms_mean'] = data['bathrooms'] - data['nb_3km_bathrooms_mean']\n",
    "            data['grade - nb_3km_grade_mean'] = data['grade'] - data['nb_3km_grade_mean']\n",
    "            data['view - nb_3km_view_mean'] = data['view'] - data['nb_3km_view_mean']\n",
    "            data['condition - nb_3km_condition_mean'] = data['condition'] - data['nb_3km_condition_mean']\n",
    "    if nb_5km:\n",
    "        neighbor_5km_stat = pd.read_csv('./neighbor_5km_stat.csv')\n",
    "        data = data.merge(neighbor_5km_stat, on='id', how='left').fillna(0)\n",
    "        \n",
    "        if original:\n",
    "            data['sqft_living - nb_5km_sqft_living_mean'] = data['sqft_living'] - data['nb_5km_sqft_living_mean']\n",
    "            data['sqft_lot - nb_5km_sqft_lot_mean'] = data['sqft_lot'] - data['nb_5km_sqft_lot_mean']\n",
    "            data['bedrooms - nb_5km_bedrooms_mean'] = data['bedrooms'] - data['nb_5km_bedrooms_mean']\n",
    "            data['bathrooms - nb_5km_bathrooms_mean'] = data['bathrooms'] - data['nb_5km_bathrooms_mean']\n",
    "            data['grade - nb_5km_grade_mean'] = data['grade'] - data['nb_5km_grade_mean']\n",
    "            data['view - nb_5km_view_mean'] = data['view'] - data['nb_5km_view_mean']\n",
    "            data['condition - nb_5km_condition_mean'] = data['condition'] - data['nb_5km_condition_mean']\n",
    "    if n_5_nb:\n",
    "        nearest_5_neighbor_stat = pd.read_csv('./nearest_5_neighbor_stat.csv')\n",
    "        data = data.merge(nearest_5_neighbor_stat, on='id', how='left').fillna(0)\n",
    "        \n",
    "        if original:\n",
    "            data['sqft_living - n_5_nb_sqft_living_mean'] = data['sqft_living'] - data['n_5_nb_sqft_living_mean']\n",
    "            data['sqft_lot - n_5_nb_sqft_lot_mean'] = data['sqft_lot'] - data['n_5_nb_sqft_lot_mean']\n",
    "            data['bedrooms - n_5_nb_bedrooms_mean'] = data['bedrooms'] - data['n_5_nb_bedrooms_mean']\n",
    "            data['bathrooms - n_5_nb_bathrooms_mean'] = data['bathrooms'] - data['n_5_nb_bathrooms_mean']\n",
    "            data['grade - n_5_nb_grade_mean'] = data['grade'] - data['n_5_nb_grade_mean']\n",
    "            data['view - n_5_nb_view_mean'] = data['view'] - data['n_5_nb_view_mean']\n",
    "            data['condition - n_5_nb_condition_mean'] = data['condition'] - data['n_5_nb_condition_mean']\n",
    "    if n_10_nb:\n",
    "        nearest_10_neighbor_stat = pd.read_csv('./nearest_10_neighbor_stat.csv')\n",
    "        data = data.merge(nearest_10_neighbor_stat, on='id', how='left').fillna(0)\n",
    "        \n",
    "        if original:\n",
    "            data['sqft_living - n_10_nb_sqft_living_mean'] = data['sqft_living'] - data['n_10_nb_sqft_living_mean']\n",
    "            data['sqft_lot - n_10_nb_sqft_lot_mean'] = data['sqft_lot'] - data['n_10_nb_sqft_lot_mean']\n",
    "            data['bedrooms - n_10_nb_bedrooms_mean'] = data['bedrooms'] - data['n_10_nb_bedrooms_mean']\n",
    "            data['bathrooms - n_10_nb_bathrooms_mean'] = data['bathrooms'] - data['n_10_nb_bathrooms_mean']\n",
    "            data['grade - n_10_nb_grade_mean'] = data['grade'] - data['n_10_nb_grade_mean']\n",
    "            data['view - n_10_nb_view_mean'] = data['view'] - data['n_10_nb_view_mean']\n",
    "            data['condition - n_10_nb_condition_mean'] = data['condition'] - data['n_10_nb_condition_mean']\n",
    "    if n_20_nb:\n",
    "        nearest_20_neighbor_stat = pd.read_csv('./nearest_20_neighbor_stat.csv')\n",
    "        data = data.merge(nearest_20_neighbor_stat, on='id', how='left').fillna(0)\n",
    "        \n",
    "        if original:\n",
    "            data['sqft_living - n_20_nb_sqft_living_mean'] = data['sqft_living'] - data['n_20_nb_sqft_living_mean']\n",
    "            data['sqft_lot - n_20_nb_sqft_lot_mean'] = data['sqft_lot'] - data['n_20_nb_sqft_lot_mean']\n",
    "            data['bedrooms - n_20_nb_bedrooms_mean'] = data['bedrooms'] - data['n_20_nb_bedrooms_mean']\n",
    "            data['bathrooms - n_20_nb_bathrooms_mean'] = data['bathrooms'] - data['n_20_nb_bathrooms_mean']\n",
    "            data['grade - n_20_nb_grade_mean'] = data['grade'] - data['n_20_nb_grade_mean']\n",
    "            data['view - n_20_nb_view_mean'] = data['view'] - data['n_20_nb_view_mean']\n",
    "            data['condition - n_20_nb_condition_mean'] = data['condition'] - data['n_20_nb_condition_mean']\n",
    "                \n",
    "    if fix_skew:\n",
    "        ordinal_cols = [\n",
    "            'id','price','data',\n",
    "            'grade','overall','view','condition','waterfront','is_renovated','has_basement',\n",
    "        ]\n",
    "        if do_ohe:\n",
    "            exclude_cols = [col for col in data.columns if 'ohe_' in col] + ordinal_cols\n",
    "        else:\n",
    "            exclude_cols = cat_cols + ordinal_cols\n",
    "        numeric_feats = [col for col in data.columns if col not in exclude_cols]\n",
    "        skewed_feats = data[numeric_feats].apply(lambda x : skew(x.dropna())).sort_values(ascending=False)\n",
    "        skewness = pd.DataFrame({'skew' :skewed_feats})\n",
    "        skewness = skewness[abs(skewness) > 0.75]\n",
    "        skewed_features = skewness.index\n",
    "        lam = 0.15\n",
    "        for feat in skewed_features:\n",
    "            data[feat] = boxcox1p(data[feat], lam)\n",
    "            data[feat] = data[feat].fillna(0)\n",
    "    \n",
    "    df = data.drop(['id','price','data'], axis=1).copy()\n",
    "\n",
    "    train_len = data[data['data'] == 'train'].shape[0]\n",
    "    X_train = df.iloc[:train_len]\n",
    "    X_test = df.iloc[train_len:]\n",
    "    y_train = data[data['data'] == 'train']['price']\n",
    "    \n",
    "    if do_scale:\n",
    "        if do_ohe:\n",
    "            non_numeric_cols = [col for col in X_train.columns if 'ohe_' in col]\n",
    "            numeric_cols = [col for col in X_train.columns if 'ohe_' not in col]\n",
    "        else:\n",
    "            non_numeric_cols = cat_cols\n",
    "            numeric_cols = [col for col in X_train.columns if col not in cat_cols]\n",
    "        X_train_rb = X_train[numeric_cols].copy()\n",
    "        X_test_rb = X_test[numeric_cols].copy()\n",
    "\n",
    "        rb = RobustScaler()\n",
    "        X_train_rb = rb.fit_transform(X_train_rb)\n",
    "        X_test_rb = rb.transform(X_test_rb)\n",
    "\n",
    "        X_train_rb = pd.DataFrame(X_train_rb, index=X_train.index, columns=X_train[numeric_cols].columns)\n",
    "        X_test_rb = pd.DataFrame(X_test_rb, index=X_test.index, columns=X_test[numeric_cols].columns)\n",
    "        \n",
    "        X_train_rb = pd.concat([X_train[non_numeric_cols], X_train_rb], axis=1)\n",
    "        X_test_rb = pd.concat([X_test[non_numeric_cols], X_test_rb], axis=1)\n",
    "        \n",
    "        print(X_train_rb.shape, X_test_rb.shape, y_train.shape)\n",
    "        \n",
    "        return X_train_rb, X_test_rb, y_train\n",
    "    else :\n",
    "        print(X_train.shape, X_test.shape, y_train.shape)\n",
    "        return X_train, X_test, y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2> prepare vatious datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:19: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "C:\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py:853: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15034, 50) (6468, 50) (15034,)\n",
      "(15034, 50) (6468, 50) (15034,)\n",
      "(15034, 50) (6468, 50) (15034,)\n",
      "(15034, 50) (6468, 50) (15034,)\n",
      "(15034, 50) (6468, 50) (15034,)\n",
      "(15034, 50) (6468, 50) (15034,)\n",
      "(15034, 392) (6468, 392) (15034,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:19: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "C:\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py:853: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15034, 734) (6468, 734) (15034,)\n"
     ]
    }
   ],
   "source": [
    "data_list = []\n",
    "\n",
    "X_train_1km, X_test_1km, y_train = load_data(nb_1km=True, nb_3km=False, nb_5km=False,\n",
    "                                             n_5_nb=False, n_10_nb=False, n_20_nb=False,\n",
    "                                             original=False, do_scale=False, do_ohe=True)\n",
    "data_list.append((X_train_1km, X_test_1km))\n",
    "\n",
    "X_train_3km, X_test_3km, y_train = load_data(nb_1km=False, nb_3km=True, nb_5km=False,\n",
    "                                             n_5_nb=False, n_10_nb=False, n_20_nb=False,\n",
    "                                             original=False, do_scale=False, do_ohe=True)\n",
    "data_list.append((X_train_3km, X_test_3km))\n",
    "\n",
    "X_train_5km, X_test_5km, y_train = load_data(nb_1km=False, nb_3km=False, nb_5km=True,\n",
    "                                             n_5_nb=False, n_10_nb=False, n_20_nb=False,\n",
    "                                             original=False, do_scale=False, do_ohe=True)\n",
    "data_list.append((X_train_5km, X_test_5km))\n",
    "\n",
    "X_train_5_nn, X_test_5_nn, y_train = load_data(nb_1km=False, nb_3km=False, nb_5km=False,\n",
    "                                               n_5_nb=True, n_10_nb=False, n_20_nb=False,\n",
    "                                               original=False, do_scale=False, do_ohe=True)\n",
    "data_list.append((X_train_5_nn, X_test_5_nn))\n",
    "\n",
    "X_train_10_nn, X_test_10_nn, y_train = load_data(nb_1km=False, nb_3km=False, nb_5km=False,\n",
    "                                                 n_5_nb=False, n_10_nb=True, n_20_nb=False,\n",
    "                                                 original=False, do_scale=False, do_ohe=True)\n",
    "data_list.append((X_train_10_nn, X_test_10_nn))\n",
    "\n",
    "X_train_20_nn, X_test_20_nn, y_train = load_data(nb_1km=False, nb_3km=False, nb_5km=False,\n",
    "                                                 n_5_nb=False, n_10_nb=False, n_20_nb=True,\n",
    "                                                 original=False, do_scale=False, do_ohe=True)\n",
    "data_list.append((X_train_20_nn, X_test_20_nn))\n",
    "\n",
    "X_train_ori, X_test_ori, y_train = load_data(nb_1km=False, nb_3km=False, nb_5km=False,\n",
    "                                             n_5_nb=False, n_10_nb=False, n_20_nb=False,\n",
    "                                             original=True, do_scale=False, do_ohe=True)\n",
    "data_list.append((X_train_ori, X_test_ori))\n",
    "\n",
    "X_train_full, X_test_full, y_train = load_data(nb_1km=True, nb_3km=True, nb_5km=True,\n",
    "                                               n_5_nb=True, n_10_nb=True, n_20_nb=True,\n",
    "                                               original=True, do_scale=False, do_ohe=True)\n",
    "data_list.append((X_train_full, X_test_full))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## 2) build various Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "StartTime:  2020-02-12 04:29:10.347379\n",
      "(15034, 50) (6468, 50) (15034,)\n",
      "\n",
      "StartTime:  2020-02-12 04:29:10.363337\n",
      "<__main__.LgbmWrapper object at 0x000001BDF984D448>\n",
      "{'objective': 'regression', 'learning_rate': 0.01, 'max_depth': 20, 'num_leaves': 63, 'min_data_in_leaf': 30, 'bagging_fraction': 0.7, 'bagging_freq': 1, 'feature_fraction': 0.2, 'seed': 0, 'metric': ['rmse']}\n",
      "\n",
      "StartTime:  2020-02-12 04:29:10.386277\n",
      "EndTime:  2020-02-12 04:29:24.830653\n",
      "TotalTime:  14.443376064300537\n",
      "Fold 1 /  CV-Score: 283812.951740\n",
      "\n",
      "StartTime:  2020-02-12 04:29:25.547736\n",
      "EndTime:  2020-02-12 04:29:37.943589\n",
      "TotalTime:  12.394857406616211\n",
      "Fold 2 /  CV-Score: 213111.955799\n",
      "\n",
      "StartTime:  2020-02-12 04:29:38.571910\n",
      "EndTime:  2020-02-12 04:29:52.239366\n",
      "TotalTime:  13.669448852539062\n",
      "Fold 3 /  CV-Score: 266915.924188\n",
      "\n",
      "StartTime:  2020-02-12 04:29:52.956447\n",
      "EndTime:  2020-02-12 04:30:04.647695\n",
      "TotalTime:  11.692244291305542\n",
      "Fold 4 /  CV-Score: 226551.203728\n",
      "\n",
      "StartTime:  2020-02-12 04:30:05.384726\n",
      "EndTime:  2020-02-12 04:30:22.173832\n",
      "TotalTime:  16.78811025619507\n",
      "Fold 5 /  CV-Score: 268152.951182\n",
      "Average CV-Score:  251708.99732726524\n",
      "\n",
      "StartTime:  2020-02-12 04:30:23.071433\n",
      "EndTime:  2020-02-12 04:30:37.521793\n",
      "TotalTime:  14.450362920761108\n",
      "EndTime:  2020-02-12 04:30:39.163404\n",
      "TotalTime:  88.80006742477417\n",
      "\n",
      "StartTime:  2020-02-12 04:30:39.170387\n",
      "<__main__.LgbmWrapper object at 0x000001BDF984D908>\n",
      "{'objective': 'regression', 'learning_rate': 0.01, 'max_depth': 10, 'num_leaves': 31, 'min_data_in_leaf': 30, 'bagging_fraction': 0.7, 'bagging_freq': 1, 'feature_fraction': 0.2, 'seed': 0, 'metric': ['rmse']}\n",
      "\n",
      "StartTime:  2020-02-12 04:30:39.188338\n",
      "EndTime:  2020-02-12 04:30:49.561599\n",
      "TotalTime:  10.35431456565857\n",
      "Fold 1 /  CV-Score: 282947.349045\n",
      "\n",
      "StartTime:  2020-02-12 04:30:50.177954\n",
      "EndTime:  2020-02-12 04:30:58.794911\n",
      "TotalTime:  8.616960048675537\n",
      "Fold 2 /  CV-Score: 213092.882108\n",
      "\n",
      "StartTime:  2020-02-12 04:30:59.284602\n",
      "EndTime:  2020-02-12 04:31:09.470365\n",
      "TotalTime:  10.18376874923706\n",
      "Fold 3 /  CV-Score: 266348.090627\n",
      "\n",
      "StartTime:  2020-02-12 04:31:10.060788\n",
      "EndTime:  2020-02-12 04:31:18.498226\n",
      "TotalTime:  8.437440156936646\n",
      "Fold 4 /  CV-Score: 226660.845312\n",
      "\n",
      "StartTime:  2020-02-12 04:31:18.978942\n",
      "EndTime:  2020-02-12 04:31:31.190290\n",
      "TotalTime:  12.21134901046753\n",
      "Fold 5 /  CV-Score: 267438.663787\n",
      "Average CV-Score:  251297.5661759954\n",
      "\n",
      "StartTime:  2020-02-12 04:31:32.262424\n",
      "EndTime:  2020-02-12 04:31:44.314197\n",
      "TotalTime:  12.051774978637695\n",
      "EndTime:  2020-02-12 04:31:45.891978\n",
      "TotalTime:  66.72159314155579\n",
      "\n",
      "StartTime:  2020-02-12 04:31:45.897964\n",
      "<__main__.LgbmWrapper object at 0x000001BDF984D748>\n",
      "{'objective': 'regression', 'learning_rate': 0.01, 'max_depth': 3, 'num_leaves': 7, 'min_data_in_leaf': 30, 'bagging_fraction': 0.7, 'bagging_freq': 1, 'feature_fraction': 0.2, 'seed': 0, 'metric': ['rmse']}\n",
      "\n",
      "StartTime:  2020-02-12 04:31:45.917910\n",
      "EndTime:  2020-02-12 04:31:52.497315\n",
      "TotalTime:  6.577412366867065\n",
      "Fold 1 /  CV-Score: 285448.406799\n",
      "\n",
      "StartTime:  2020-02-12 04:31:52.967060\n",
      "EndTime:  2020-02-12 04:31:59.031844\n",
      "TotalTime:  6.063786745071411\n",
      "Fold 2 /  CV-Score: 213181.716283\n",
      "\n",
      "StartTime:  2020-02-12 04:31:59.429780\n",
      "EndTime:  2020-02-12 04:32:06.866893\n",
      "TotalTime:  7.436115980148315\n",
      "Fold 3 /  CV-Score: 266383.956771\n",
      "\n",
      "StartTime:  2020-02-12 04:32:07.519152\n",
      "EndTime:  2020-02-12 04:32:13.340583\n",
      "TotalTime:  5.819439172744751\n",
      "Fold 4 /  CV-Score: 225053.832332\n",
      "\n",
      "StartTime:  2020-02-12 04:32:13.708600\n",
      "EndTime:  2020-02-12 04:32:21.317254\n",
      "TotalTime:  7.605663776397705\n",
      "Fold 5 /  CV-Score: 266718.618660\n",
      "Average CV-Score:  251357.3061690103\n",
      "\n",
      "StartTime:  2020-02-12 04:32:22.009405\n",
      "EndTime:  2020-02-12 04:32:30.132683\n",
      "TotalTime:  8.123279333114624\n",
      "EndTime:  2020-02-12 04:32:31.407275\n",
      "TotalTime:  45.51031041145325\n",
      "\n",
      "StartTime:  2020-02-12 04:32:31.416252\n",
      "<__main__.XgbWrapper object at 0x000001BDF984DAC8>\n",
      "{'eval_metric': 'rmse', 'seed': 0, 'eta': 0.01, 'max_depth': 20, 'subsample': 0.7, 'colsample_bytree': 0.5, 'silent': 1}\n",
      "\n",
      "StartTime:  2020-02-12 04:32:31.434204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\xgboost\\core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EndTime:  2020-02-12 04:34:43.875069\n",
      "TotalTime:  132.44086694717407\n",
      "Fold 1 /  CV-Score: 278475.012198\n",
      "\n",
      "StartTime:  2020-02-12 04:34:45.713155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\xgboost\\core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EndTime:  2020-02-12 04:36:31.456407\n",
      "TotalTime:  105.74225664138794\n",
      "Fold 2 /  CV-Score: 212078.050608\n",
      "\n",
      "StartTime:  2020-02-12 04:36:33.071091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\xgboost\\core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EndTime:  2020-02-12 04:38:51.756784\n",
      "TotalTime:  138.6817045211792\n",
      "Fold 3 /  CV-Score: 263605.420465\n",
      "\n",
      "StartTime:  2020-02-12 04:38:54.193271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\xgboost\\core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EndTime:  2020-02-12 04:40:13.308802\n",
      "TotalTime:  79.11352705955505\n",
      "Fold 4 /  CV-Score: 223665.571311\n",
      "\n",
      "StartTime:  2020-02-12 04:40:14.595351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\xgboost\\core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EndTime:  2020-02-12 04:43:13.211747\n",
      "TotalTime:  178.62139201164246\n",
      "Fold 5 /  CV-Score: 264510.261706\n",
      "Average CV-Score:  248466.8632575716\n",
      "\n",
      "StartTime:  2020-02-12 04:43:16.006276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\xgboost\\core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "C:\\Anaconda3\\lib\\site-packages\\xgboost\\core.py:588: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  data.base is not None and isinstance(data, np.ndarray) \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EndTime:  2020-02-12 04:46:24.197613\n",
      "TotalTime:  188.19133877754211\n",
      "EndTime:  2020-02-12 04:46:28.508088\n",
      "TotalTime:  837.0988185405731\n",
      "\n",
      "StartTime:  2020-02-12 04:46:28.522053\n",
      "<__main__.XgbWrapper object at 0x000001BDF984D9C8>\n",
      "{'eval_metric': 'rmse', 'seed': 0, 'eta': 0.01, 'max_depth': 10, 'subsample': 0.7, 'colsample_bytree': 0.5, 'silent': 1}\n",
      "\n",
      "StartTime:  2020-02-12 04:46:28.552968\n",
      "EndTime:  2020-02-12 04:47:26.175889\n",
      "TotalTime:  57.62292194366455\n",
      "Fold 1 /  CV-Score: 278143.088265\n",
      "\n",
      "StartTime:  2020-02-12 04:47:26.653613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\xgboost\\core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EndTime:  2020-02-12 04:48:01.123444\n",
      "TotalTime:  34.46983098983765\n",
      "Fold 2 /  CV-Score: 211281.990303\n",
      "\n",
      "StartTime:  2020-02-12 04:48:01.467524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\xgboost\\core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EndTime:  2020-02-12 04:48:50.869427\n",
      "TotalTime:  49.40190267562866\n",
      "Fold 3 /  CV-Score: 260502.793026\n",
      "\n",
      "StartTime:  2020-02-12 04:48:51.300277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\xgboost\\core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EndTime:  2020-02-12 04:49:28.170688\n",
      "TotalTime:  36.86841654777527\n",
      "Fold 4 /  CV-Score: 222379.789929\n",
      "\n",
      "StartTime:  2020-02-12 04:49:28.566631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\xgboost\\core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EndTime:  2020-02-12 04:50:51.278066\n",
      "TotalTime:  82.71644139289856\n",
      "Fold 5 /  CV-Score: 262515.884450\n",
      "Average CV-Score:  246964.70919458196\n",
      "\n",
      "StartTime:  2020-02-12 04:50:52.421988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\xgboost\\core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "C:\\Anaconda3\\lib\\site-packages\\xgboost\\core.py:588: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  data.base is not None and isinstance(data, np.ndarray) \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EndTime:  2020-02-12 04:52:30.929166\n",
      "TotalTime:  98.50313663482666\n",
      "EndTime:  2020-02-12 04:52:32.709406\n",
      "TotalTime:  364.1873559951782\n",
      "\n",
      "StartTime:  2020-02-12 04:52:32.723370\n",
      "<__main__.XgbWrapper object at 0x000001BDF984D488>\n",
      "{'eval_metric': 'rmse', 'seed': 0, 'eta': 0.01, 'max_depth': 3, 'subsample': 0.7, 'colsample_bytree': 0.5, 'silent': 1}\n",
      "\n",
      "StartTime:  2020-02-12 04:52:32.749301\n",
      "EndTime:  2020-02-12 04:53:29.195584\n",
      "TotalTime:  56.44635987281799\n",
      "Fold 1 /  CV-Score: 281283.110399\n",
      "\n",
      "StartTime:  2020-02-12 04:53:29.387695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\xgboost\\core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EndTime:  2020-02-12 04:54:08.610363\n",
      "TotalTime:  39.22380042076111\n",
      "Fold 2 /  CV-Score: 211819.782859\n",
      "\n",
      "StartTime:  2020-02-12 04:54:08.745011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\xgboost\\core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EndTime:  2020-02-12 04:55:34.854182\n",
      "TotalTime:  86.10918831825256\n",
      "Fold 3 /  CV-Score: 261110.533569\n",
      "\n",
      "StartTime:  2020-02-12 04:55:35.178250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\xgboost\\core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EndTime:  2020-02-12 04:56:20.967896\n",
      "TotalTime:  45.78567576408386\n",
      "Fold 4 /  CV-Score: 223646.096131\n",
      "\n",
      "StartTime:  2020-02-12 04:56:21.174324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\xgboost\\core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EndTime:  2020-02-12 04:57:24.277495\n",
      "TotalTime:  63.1032280921936\n",
      "Fold 5 /  CV-Score: 263589.337202\n",
      "Average CV-Score:  248289.7720318418\n",
      "\n",
      "StartTime:  2020-02-12 04:57:24.611821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\xgboost\\core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "C:\\Anaconda3\\lib\\site-packages\\xgboost\\core.py:588: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  data.base is not None and isinstance(data, np.ndarray) \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EndTime:  2020-02-12 04:58:54.022938\n",
      "TotalTime:  89.41125988960266\n",
      "EndTime:  2020-02-12 04:58:54.627250\n",
      "TotalTime:  381.90387868881226\n",
      "\n",
      "StartTime:  2020-02-12 04:58:54.637224\n",
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=20,\n",
      "                      max_features=0.6, max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
      "                      n_jobs=-1, oob_score=False, random_state=0, verbose=0,\n",
      "                      warm_start=False)\n",
      "\n",
      "StartTime:  2020-02-12 04:58:54.665147\n",
      "EndTime:  2020-02-12 05:01:12.762679\n",
      "TotalTime:  138.1045126914978\n",
      "Fold 1 /  CV-Score: 274842.524887\n",
      "\n",
      "StartTime:  2020-02-12 05:01:13.423686\n",
      "EndTime:  2020-02-12 05:03:28.984225\n",
      "TotalTime:  135.55780744552612\n",
      "Fold 2 /  CV-Score: 210963.900888\n",
      "\n",
      "StartTime:  2020-02-12 05:03:29.535030\n",
      "EndTime:  2020-02-12 05:05:40.356354\n",
      "TotalTime:  130.82333326339722\n",
      "Fold 3 /  CV-Score: 258178.430286\n",
      "\n",
      "StartTime:  2020-02-12 05:05:41.034542\n",
      "EndTime:  2020-02-12 05:08:00.697382\n",
      "TotalTime:  139.66814064979553\n",
      "Fold 4 /  CV-Score: 218460.548912\n",
      "\n",
      "StartTime:  2020-02-12 05:08:01.493305\n",
      "EndTime:  2020-02-12 05:10:14.929575\n",
      "TotalTime:  133.4363248348236\n",
      "Fold 5 /  CV-Score: 262649.506325\n",
      "Average CV-Score:  245018.98225965892\n",
      "\n",
      "StartTime:  2020-02-12 05:10:15.681564\n",
      "EndTime:  2020-02-12 05:13:19.185581\n",
      "TotalTime:  183.51299285888672\n",
      "EndTime:  2020-02-12 05:13:20.506635\n",
      "TotalTime:  865.874400138855\n",
      "\n",
      "StartTime:  2020-02-12 05:13:20.521597\n",
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=3,\n",
      "                      max_features=0.6, max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
      "                      n_jobs=-1, oob_score=False, random_state=0, verbose=0,\n",
      "                      warm_start=False)\n",
      "\n",
      "StartTime:  2020-02-12 05:13:20.554508\n",
      "EndTime:  2020-02-12 05:13:49.316442\n",
      "TotalTime:  28.75917077064514\n",
      "Fold 1 /  CV-Score: 301204.382739\n",
      "\n",
      "StartTime:  2020-02-12 05:13:49.771541\n",
      "EndTime:  2020-02-12 05:14:18.570589\n",
      "TotalTime:  28.805137634277344\n",
      "Fold 2 /  CV-Score: 224454.779753\n",
      "\n",
      "StartTime:  2020-02-12 05:14:19.020385\n",
      "EndTime:  2020-02-12 05:14:47.383185\n",
      "TotalTime:  28.36406636238098\n",
      "Fold 3 /  CV-Score: 291357.356564\n",
      "\n",
      "StartTime:  2020-02-12 05:14:47.727232\n",
      "EndTime:  2020-02-12 05:15:15.461916\n",
      "TotalTime:  27.7372727394104\n",
      "Fold 4 /  CV-Score: 240303.331112\n",
      "\n",
      "StartTime:  2020-02-12 05:15:15.906512\n",
      "EndTime:  2020-02-12 05:15:44.156972\n",
      "TotalTime:  28.249377489089966\n",
      "Fold 5 /  CV-Score: 280552.695048\n",
      "Average CV-Score:  267574.5090430834\n",
      "\n",
      "StartTime:  2020-02-12 05:15:44.730442\n",
      "EndTime:  2020-02-12 05:16:19.373983\n",
      "TotalTime:  34.64341139793396\n",
      "EndTime:  2020-02-12 05:16:19.895459\n",
      "TotalTime:  179.37577414512634\n",
      "\n",
      "StartTime:  2020-02-12 05:16:19.915407\n",
      "Ridge(alpha=0.001, copy_X=True, fit_intercept=True, max_iter=10000000.0,\n",
      "      normalize=True, random_state=0, solver='auto', tol=0.001)\n",
      "\n",
      "StartTime:  2020-02-12 05:16:19.979237\n",
      "EndTime:  2020-02-12 05:16:20.070118\n",
      "TotalTime:  0.09175300598144531\n",
      "Fold 1 /  CV-Score: 340753.078922\n",
      "\n",
      "StartTime:  2020-02-12 05:16:20.121876\n",
      "EndTime:  2020-02-12 05:16:20.146786\n",
      "TotalTime:  0.025824785232543945\n",
      "Fold 2 /  CV-Score: 266211.678679\n",
      "\n",
      "StartTime:  2020-02-12 05:16:20.191666\n",
      "EndTime:  2020-02-12 05:16:20.216599\n",
      "TotalTime:  0.024933815002441406\n",
      "Fold 3 /  CV-Score: 325126.060304\n",
      "\n",
      "StartTime:  2020-02-12 05:16:20.258521\n",
      "EndTime:  2020-02-12 05:16:20.283421\n",
      "TotalTime:  0.030855894088745117\n",
      "Fold 4 /  CV-Score: 291605.534471\n",
      "\n",
      "StartTime:  2020-02-12 05:16:20.327396\n",
      "EndTime:  2020-02-12 05:16:20.353354\n",
      "TotalTime:  0.02792644500732422\n",
      "Fold 5 /  CV-Score: 312697.306247\n",
      "Average CV-Score:  307278.7317244598\n",
      "\n",
      "StartTime:  2020-02-12 05:16:20.489869\n",
      "EndTime:  2020-02-12 05:16:20.524846\n",
      "TotalTime:  0.03569507598876953\n",
      "EndTime:  2020-02-12 05:16:20.543725\n",
      "TotalTime:  0.6230525970458984\n",
      "(15034, 50) (6468, 50) (15034,)\n",
      "\n",
      "StartTime:  2020-02-12 05:16:20.561677\n",
      "<lightgbm.basic.Booster object at 0x000001BDF990A988>\n",
      "\n",
      "StartTime:  2020-02-12 05:16:20.597582\n",
      "EndTime:  2020-02-12 05:16:42.472611\n",
      "TotalTime:  21.874955892562866\n",
      "Fold 1 /  CV-Score: 277758.904434\n",
      "\n",
      "StartTime:  2020-02-12 05:16:43.637940\n"
     ]
    }
   ],
   "source": [
    "model_list = []\n",
    "\n",
    "lgb_param = {\n",
    "    'objective': 'regression',\n",
    "    'learning_rate': 0.01,\n",
    "    'max_depth': 20,\n",
    "    'num_leaves': 63,\n",
    "    'min_data_in_leaf': 30,\n",
    "    'bagging_fraction': 0.7,\n",
    "    'bagging_freq': 1,\n",
    "    'feature_fraction': 0.2,\n",
    "    'seed': RANDOM_SEED,\n",
    "    'metric': ['rmse'],\n",
    "}\n",
    "lgb_model = LgbmWrapper(params=lgb_param, num_rounds=100000, ealry_stopping=200, verbose_eval=False)\n",
    "model_list.append(lgb_model)\n",
    "\n",
    "lgb_param2 = {\n",
    "    'objective': 'regression',\n",
    "    'learning_rate': 0.01,\n",
    "    'max_depth': 10,\n",
    "    'num_leaves': 31,\n",
    "    'min_data_in_leaf': 30,\n",
    "    'bagging_fraction': 0.7,\n",
    "    'bagging_freq': 1,\n",
    "    'feature_fraction': 0.2,\n",
    "    'seed': RANDOM_SEED,\n",
    "    'metric': ['rmse'],\n",
    "}\n",
    "lgb_model2 = LgbmWrapper(params=lgb_param2, num_rounds=100000, ealry_stopping=200, verbose_eval=False)\n",
    "model_list.append(lgb_model2)\n",
    "\n",
    "lgb_param3 = {\n",
    "    'objective': 'regression',\n",
    "    'learning_rate': 0.01,\n",
    "    'max_depth': 3,\n",
    "    'num_leaves': 7,\n",
    "    'min_data_in_leaf': 30,\n",
    "    'bagging_fraction': 0.7,\n",
    "    'bagging_freq': 1,\n",
    "    'feature_fraction': 0.2,\n",
    "    'seed': RANDOM_SEED,\n",
    "    'metric': ['rmse'],\n",
    "}\n",
    "lgb_model3 = LgbmWrapper(params=lgb_param3, num_rounds=100000, ealry_stopping=200, verbose_eval=False)\n",
    "model_list.append(lgb_model3)\n",
    "\n",
    "xgb_param = {\n",
    "    'eval_metric': 'rmse',\n",
    "    'seed': RANDOM_SEED,\n",
    "    'eta': 0.01,\n",
    "    'max_depth': 20,\n",
    "    'subsample': 0.7,\n",
    "    'colsample_bytree': 0.5,\n",
    "    'silent': 1,\n",
    "}\n",
    "xgb_model = XgbWrapper(params=xgb_param, num_rounds=100000, ealry_stopping=200, verbose_eval=False)\n",
    "model_list.append(xgb_model)\n",
    "\n",
    "xgb_param2 = {\n",
    "    'eval_metric': 'rmse',\n",
    "    'seed': RANDOM_SEED,\n",
    "    'eta': 0.01,\n",
    "    'max_depth': 10,\n",
    "    'subsample': 0.7,\n",
    "    'colsample_bytree': 0.5,\n",
    "    'silent': 1,\n",
    "}\n",
    "xgb_model2 = XgbWrapper(params=xgb_param2, num_rounds=100000, ealry_stopping=200, verbose_eval=False)\n",
    "model_list.append(xgb_model2)\n",
    "\n",
    "xgb_param3 = {\n",
    "    'eval_metric': 'rmse',\n",
    "    'seed': RANDOM_SEED,\n",
    "    'eta': 0.01,\n",
    "    'max_depth': 3,\n",
    "    'subsample': 0.7,\n",
    "    'colsample_bytree': 0.5,\n",
    "    'silent': 1,\n",
    "}\n",
    "xgb_model3 = XgbWrapper(params=xgb_param3, num_rounds=100000, ealry_stopping=200, verbose_eval=False)\n",
    "model_list.append(xgb_model3)\n",
    "\n",
    "rf_param = {\n",
    "    'n_estimators': 1000,\n",
    "    'max_depth': 20,\n",
    "    'max_features': 0.6,\n",
    "    'n_jobs': -1,\n",
    "    'random_state': RANDOM_SEED\n",
    "}\n",
    "rf_model = SklearnWrapper(RandomForestRegressor, params=rf_param)\n",
    "model_list.append(rf_model)\n",
    "\n",
    "rf_param2 = {\n",
    "    'n_estimators': 1000,\n",
    "    'max_depth': 3,\n",
    "    'max_features': 0.6,\n",
    "    'n_jobs': -1,\n",
    "    'random_state': RANDOM_SEED\n",
    "}\n",
    "rf_model2 = SklearnWrapper(RandomForestRegressor, params=rf_param2)\n",
    "model_list.append(rf_model2)\n",
    "\n",
    "ridge_param = {'alpha': 1e-3, 'normalize': True, 'max_iter': 1e7, 'random_state': RANDOM_SEED}\n",
    "ridge_model = SklearnWrapper(Ridge, params=ridge_param)\n",
    "model_list.append(ridge_model)\n",
    "\n",
    "X_train_single, X_test_single, cv_score_single = stacking(data_list, y_train, model_list, eval_func=rmse_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_set_list = ['nb_1km','nb_3km','nb_5km','nn_5','nn_10','nn_20','original','full']\n",
    "model_name_list = ['lgb_high','lgb_mid','lgb_low','xgb_high','xgb_mid','xgb_low','rf_high','rf_low','ridge']\n",
    "result = []\n",
    "for feature_set in feature_set_list:\n",
    "    for model_name in model_name_list:\n",
    "        result.append(feature_set + ', ' + model_name)\n",
    "\n",
    "cv_score_single_df = pd.DataFrame({\n",
    "    'name': result,\n",
    "    'Single Model CV Score':cv_score_single\n",
    "})\n",
    "\n",
    "cv_score_single_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_score_single_df.set_index('name').plot.bar(figsize=(16,8));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_single.shape, X_test_single.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_single.to_csv('./x_train_single.csv', index=False)\n",
    "X_test_single.to_csv('./x_test_single.csv', index=False)\n",
    "cv_score_single_df.to_csv('./cv_score_single.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_stage1 = pd.concat([\n",
    "    pd.read_csv('./x_train_single.csv'),\n",
    "    X_train_full[['nb_1km_price_mean','nb_3km_price_mean','nb_5km_price_mean',\n",
    "                  'n_5_nb_price_mean','n_10_nb_price_mean','n_20_nb_price_mean']]\n",
    "], axis=1)\n",
    "\n",
    "X_test_stage1 = pd.concat([\n",
    "    pd.read_csv('../x_test_single.csv'),\n",
    "    X_test_full[['nb_1km_price_mean','nb_3km_price_mean','nb_5km_price_mean',\n",
    "                 'n_5_nb_price_mean','n_10_nb_price_mean','n_20_nb_price_mean']].reset_index(drop=True)\n",
    "], axis=1)\n",
    "\n",
    "X_train_stage1.shape, X_test_stage1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, activation='selu', input_dim=input_dim,\n",
    "                    kernel_initializer=initializers.he_normal(seed=RANDOM_SEED),\n",
    "                    bias_initializer=initializers.Constant(0.01)))\n",
    "    model.add(Dense(32, activation='selu', \n",
    "                    kernel_initializer=initializers.he_normal(seed=RANDOM_SEED),\n",
    "                    bias_initializer=initializers.Constant(0.01)))\n",
    "    model.add(Dense(16, activation='selu',\n",
    "                    kernel_initializer=initializers.he_normal(seed=RANDOM_SEED),\n",
    "                    bias_initializer=initializers.Constant(0.01)))\n",
    "    model.add(Dense(8, activation='selu',\n",
    "                    kernel_initializer=initializers.he_normal(seed=RANDOM_SEED),\n",
    "                    bias_initializer=initializers.Constant(0.01)))\n",
    "    model.add(Dense(1,\n",
    "                    kernel_initializer=initializers.he_normal(seed=RANDOM_SEED),\n",
    "                    bias_initializer=initializers.Constant(0.01)))\n",
    "\n",
    "\n",
    "    optimizer = optimizers.RMSprop(lr=0.001)\n",
    "    model.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['mse'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "patient = 200\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=patient, mode='min', verbose=1),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=patient/2, min_lr=0.00001, verbose=1, mode='min')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list_second = []\n",
    "\n",
    "keras_model = KerasWrapper(create_model, epochs=100000, batch_size=512, callbacks=callbacks, use_avg_oof=True)\n",
    "model_list_second.append(keras_model)\n",
    "\n",
    "et_param = {\n",
    "    'n_estimators': 1000,\n",
    "    'max_depth': 20,\n",
    "    'max_features': 0.6,\n",
    "    'n_jobs': -1,\n",
    "    'random_state': RANDOM_SEED\n",
    "}\n",
    "et_model = SklearnWrapper(ExtraTreesRegressor, params=et_param)\n",
    "model_list_second.append(et_model)\n",
    "\n",
    "et_param2 = {\n",
    "    'n_estimators': 1000,\n",
    "    'max_depth': 3,\n",
    "    'max_features': 0.6,\n",
    "    'n_jobs': -1,\n",
    "    'random_state': RANDOM_SEED\n",
    "}\n",
    "et_model2 = SklearnWrapper(ExtraTreesRegressor, params=et_param2)\n",
    "model_list_second.append(et_model2)\n",
    "\n",
    "lgb_param = {\n",
    "    'objective': 'regression',\n",
    "    'learning_rate': 0.01,\n",
    "    'max_depth': 20,\n",
    "    'num_leaves': 63,\n",
    "    'min_data_in_leaf': 30,\n",
    "    'bagging_fraction': 0.7,\n",
    "    'bagging_freq': 1,\n",
    "    'feature_fraction': 0.2,\n",
    "    'seed': RANDOM_SEED,\n",
    "    'metric': ['rmse'],\n",
    "}\n",
    "lgb_model = LgbmWrapper(params=lgb_param, num_rounds=100000, ealry_stopping=200, verbose_eval=False)\n",
    "model_list_second.append(lgb_model)\n",
    "\n",
    "lgb_param2 = {\n",
    "    'objective': 'regression',\n",
    "    'learning_rate': 0.01,\n",
    "    'max_depth': 10,\n",
    "    'num_leaves': 31,\n",
    "    'min_data_in_leaf': 30,\n",
    "    'bagging_fraction': 0.7,\n",
    "    'bagging_freq': 1,\n",
    "    'feature_fraction': 0.2,\n",
    "    'seed': RANDOM_SEED,\n",
    "    'metric': ['rmse'],\n",
    "}\n",
    "lgb_model2 = LgbmWrapper(params=lgb_param2, num_rounds=100000, ealry_stopping=200, verbose_eval=False)\n",
    "model_list_second.append(lgb_model2)\n",
    "\n",
    "lgb_param3 = {\n",
    "    'objective': 'regression',\n",
    "    'learning_rate': 0.01,\n",
    "    'max_depth': 3,\n",
    "    'num_leaves': 7,\n",
    "    'min_data_in_leaf': 30,\n",
    "    'bagging_fraction': 0.7,\n",
    "    'bagging_freq': 1,\n",
    "    'feature_fraction': 0.2,\n",
    "    'seed': RANDOM_SEED,\n",
    "    'metric': ['rmse'],\n",
    "}\n",
    "lgb_model3 = LgbmWrapper(params=lgb_param3, num_rounds=100000, ealry_stopping=200, verbose_eval=False)\n",
    "model_list_second.append(lgb_model3)\n",
    "\n",
    "xgb_param = {\n",
    "    'eval_metric': 'rmse',\n",
    "    'seed': RANDOM_SEED,\n",
    "    'eta': 0.01,\n",
    "    'max_depth': 20,\n",
    "    'subsample': 0.7,\n",
    "    'colsample_bytree': 0.5,\n",
    "    'silent': 1,\n",
    "}\n",
    "xgb_model = XgbWrapper(params=xgb_param, num_rounds=100000, ealry_stopping=200, verbose_eval=False)\n",
    "model_list_second.append(xgb_model)\n",
    "\n",
    "xgb_param2 = {\n",
    "    'eval_metric': 'rmse',\n",
    "    'seed': RANDOM_SEED,\n",
    "    'eta': 0.01,\n",
    "    'max_depth': 10,\n",
    "    'subsample': 0.7,\n",
    "    'colsample_bytree': 0.5,\n",
    "    'silent': 1,\n",
    "}\n",
    "xgb_model2 = XgbWrapper(params=xgb_param2, num_rounds=100000, ealry_stopping=200, verbose_eval=False)\n",
    "model_list_second.append(xgb_model2)\n",
    "\n",
    "xgb_param3 = {\n",
    "    'eval_metric': 'rmse',\n",
    "    'seed': RANDOM_SEED,\n",
    "    'eta': 0.01,\n",
    "    'max_depth': 3,\n",
    "    'subsample': 0.7,\n",
    "    'colsample_bytree': 0.5,\n",
    "    'silent': 1,\n",
    "}\n",
    "xgb_model3 = XgbWrapper(params=xgb_param3, num_rounds=100000, ealry_stopping=200, verbose_eval=False)\n",
    "model_list_second.append(xgb_model3)\n",
    "\n",
    "rf_param = {\n",
    "    'n_estimators': 1000,\n",
    "    'max_depth': 20,\n",
    "    'max_features': 0.6,\n",
    "    'n_jobs': -1,\n",
    "    'random_state': RANDOM_SEED\n",
    "}\n",
    "rf_model = SklearnWrapper(RandomForestRegressor, params=rf_param)\n",
    "model_list_second.append(rf_model)\n",
    "\n",
    "rf_param2 = {\n",
    "    'n_estimators': 1000,\n",
    "    'max_depth': 3,\n",
    "    'max_features': 0.6,\n",
    "    'n_jobs': -1,\n",
    "    'random_state': RANDOM_SEED\n",
    "}\n",
    "rf_model2 = SklearnWrapper(RandomForestRegressor, params=rf_param2)\n",
    "model_list_second.append(rf_model2)\n",
    "\n",
    "ridge_param = {'alpha': 1e-10, 'normalize': True, 'max_iter': 1e7, 'random_state': RANDOM_SEED}\n",
    "ridge_model = SklearnWrapper(Ridge, params=ridge_param)\n",
    "model_list_second.append(ridge_model)\n",
    "\n",
    "gbr_param = {\n",
    "    'n_estimators': 1000,\n",
    "    'learning_rate':0.1,\n",
    "    'max_depth': 20,\n",
    "    'subsample': 0.7,\n",
    "    'max_features': 0.6,\n",
    "    'random_state': RANDOM_SEED\n",
    "}\n",
    "gbr_model = SklearnWrapper(GradientBoostingRegressor, params=gbr_param)\n",
    "model_list_second.append(gbr_model)\n",
    "\n",
    "gbr_param2 = {\n",
    "    'n_estimators': 1000,\n",
    "    'learning_rate':0.1,\n",
    "    'max_depth': 10,\n",
    "    'subsample': 0.7,\n",
    "    'max_features': 0.6,\n",
    "    'random_state': RANDOM_SEED\n",
    "}\n",
    "gbr_model2 = SklearnWrapper(GradientBoostingRegressor, params=gbr_param2)\n",
    "model_list_second.append(gbr_model2)\n",
    "\n",
    "gbr_param3 = {\n",
    "    'n_estimators': 1000,\n",
    "    'learning_rate':0.1,\n",
    "    'max_depth': 2,\n",
    "    'subsample': 0.7,\n",
    "    'max_features': 0.6,\n",
    "    'random_state': RANDOM_SEED\n",
    "}\n",
    "gbr_model3 = SklearnWrapper(GradientBoostingRegressor, params=gbr_param3)\n",
    "model_list_second.append(gbr_model3)\n",
    "\n",
    "lasso_param = {'alpha':1e-6, 'normalize':True, 'max_iter':1e7, 'random_state':RANDOM_SEED}\n",
    "lasso_model = SklearnWrapper(Lasso, params=lasso_param)\n",
    "model_list_second.append(lasso_model)\n",
    "\n",
    "elastic_param = {'alpha':1e-6, 'normalize':True, 'max_iter':1e5, 'random_state':RANDOM_SEED, 'l1_ratio':0.8}\n",
    "elastic_model = SklearnWrapper(ElasticNet, params=elastic_param)\n",
    "model_list_second.append(elastic_model)\n",
    "\n",
    "svr_param = {'C':1e3, 'epsilon':0.001, 'gamma':1e-4}\n",
    "svr_model = SklearnWrapper(SVR, params=svr_param)\n",
    "model_list_second.append(svr_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_list_stage1 = [(X_train_stage1, X_test_stage1)]\n",
    "X_train_stage2, X_test_stage2, cv_score_stage1 = stacking(data_list_stage1, y_train, model_list_second,\n",
    "                                                          eval_func=rmse_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage1_model_name_list = ['nn','ext_high','ext_low','lgb_high','lgb_mid','lgb_low','xgb_high','xgb_mid','xgb_low',\n",
    "                   'rf_high','rf_low','ridge','gbr_high','gbr_mid','gbr_low','lasso','elastic','svr']\n",
    "\n",
    "cv_score_stage1_df = pd.DataFrame({\n",
    "    'name': stage1_model_name_list,\n",
    "    'Stage 1 CV Score':cv_score_stage1\n",
    "})\n",
    "\n",
    "cv_score_stage1_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_score_stage1_df.set_index('name').plot.bar(figsize=(16,8));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_stage2.to_csv('./x_train_stage2.csv', index=False)\n",
    "X_test_stage2.to_csv('./x_test_stage2.csv', index=False)\n",
    "cv_score_stage1_df.to_csv('./cv_score_stage1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_stage2 = pd.read_csv('./x_train_stage2.csv')\n",
    "X_test_stage2 = pd.read_csv('./x_test_stage2.csv')\n",
    "cv_score_stage1_df = pd.read_csv('./cv_score_stage1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_l3_param = {\n",
    "    'alpha': 1e-10,\n",
    "    'normalize': True,\n",
    "    'max_iter': 1e7,\n",
    "    'random_state': RANDOM_SEED\n",
    "}\n",
    "\n",
    "ridge_l3_model = SklearnWrapper(Ridge, params=ridge_l3_param, use_avg_oof=True)\n",
    "\n",
    "ridge_l3_train, ridge_l3_test, ridge_l3_cv_score = get_oof(ridge_l3_model, X_train_stage2, y_train, X_test_stage2,\n",
    "                                                           rmse_exp, NFOLDS=5, kfold_random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_l3_param = {'alpha':1e-7, 'normalize':True, 'max_iter':1e7, 'random_state':RANDOM_SEED}\n",
    "\n",
    "lasso_l3_model = SklearnWrapper(Lasso, params=lasso_l3_param, use_avg_oof=True)\n",
    "lasso_l3_train, lasso_l3_test, lasso_l3_cv_score = get_oof(lasso_l3_model, X_train_stage2, y_train, X_test_stage2,\n",
    "                                                  rmse_exp, NFOLDS=5, kfold_random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elastic_l3_param = {'alpha':1e-8, 'normalize':True, 'max_iter':1e6, 'random_state':RANDOM_SEED, 'l1_ratio':0.8}\n",
    "\n",
    "elastic_l3_model = SklearnWrapper(ElasticNet, params=elastic_l3_param, use_avg_oof=True)\n",
    "elastic_l3_train, elastic_l3_test, elastic_l3_cv_score = get_oof(elastic_l3_model, X_train_stage2, y_train, X_test_stage2,\n",
    "                                                      rmse_exp, NFOLDS=5, kfold_random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_l3_param = {'C':1e3, 'epsilon':0.001, 'gamma':1e-4}\n",
    "\n",
    "svr_l3_model = SklearnWrapper(SVR, params=svr_l3_param, use_avg_oof=True)\n",
    "svr_l3_train, svr_l3_test, svr_l3_cv_score = get_oof(svr_l3_model, X_train_stage2, y_train, X_test_stage2,\n",
    "                                                     rmse_exp, NFOLDS=5, kfold_random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_score_stage2_df = pd.DataFrame({\n",
    "    'name': ['ridge','lasso','elastic','svr'],\n",
    "    'Stage 2 CV Score':[ridge_l3_cv_score,lasso_l3_cv_score,elastic_l3_cv_score,svr_l3_cv_score]\n",
    "})\n",
    "cv_score_stage2_df.to_csv('./cv_score_stage2.csv', index=False)\n",
    "\n",
    "cv_score_stage2_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_score_stage2_df.set_index('name').plot.bar(figsize=(16,8));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_pred = elastic_l3_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# public leaderboard score: 98316.65734\n",
    "\n",
    "test = pd.read_csv('./test.csv')\n",
    "\n",
    "output = f'stacking_{datetime.now().strftime(\"%Y%m%d%H%M%S\")}.csv'\n",
    "print(output)\n",
    "\n",
    "submission = pd.DataFrame({'id': test['id'], 'price': np.expm1(avg_pred.ravel())})\n",
    "submission.to_csv(output, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
